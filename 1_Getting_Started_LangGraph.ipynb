{"cells":[{"cell_type":"markdown","metadata":{"id":"qsFqqI3E8Xn4"},"source":["## Chatbots With Langgraph"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12974,"status":"ok","timestamp":1725413942997,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"aDGN7B1e8cX_","outputId":"2d56dab8-f754-46b6-ca45-47200862ef12"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langgraph\n","  Downloading langgraph-0.2.16-py3-none-any.whl.metadata (13 kB)\n","Collecting langsmith\n","  Downloading langsmith-0.1.111-py3-none-any.whl.metadata (13 kB)\n","Collecting langchain-core<0.3,>=0.2.27 (from langgraph)\n","  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n","Collecting langgraph-checkpoint<2.0.0,>=1.0.2 (from langgraph)\n","  Downloading langgraph_checkpoint-1.0.8-py3-none-any.whl.metadata (4.5 kB)\n","Collecting httpx<1,>=0.23.0 (from langsmith)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langsmith) (2.8.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith) (2.32.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (2024.7.4)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.8)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (6.0.2)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.2.27->langgraph)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (24.1)\n","Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3,>=0.2.27->langgraph)\n","  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (4.12.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langsmith) (2.20.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (2.0.7)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.27->langgraph)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.2.2)\n","Downloading langgraph-0.2.16-py3-none-any.whl (91 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langsmith-0.1.111-py3-none-any.whl (288 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.5/288.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_checkpoint-1.0.8-py3-none-any.whl (15 kB)\n","Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Installing collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langgraph-checkpoint, langgraph\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.38 langgraph-0.2.16 langgraph-checkpoint-1.0.8 langsmith-0.1.111 orjson-3.10.7 tenacity-8.5.0\n"]}],"source":["!pip install langgraph langsmith"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7768,"status":"ok","timestamp":1725414059086,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"uOSj_MR1AHBs","outputId":"5d21aed8-ef5b-4558-e456-bbbf571cdabf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain\n","  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain_groq\n","  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\n","Collecting langchain_community\n","  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.38)\n","Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n","  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.111)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n","Collecting groq<1,>=0.4.1 (from langchain_groq)\n","  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\n","Downloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n","Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, typing-inspect, groq, dataclasses-json, langchain-text-splitters, langchain_groq, langchain, langchain_community\n","Successfully installed dataclasses-json-0.6.7 groq-0.11.0 langchain-0.2.16 langchain-text-splitters-0.2.4 langchain_community-0.2.16 langchain_groq-0.1.9 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"]}],"source":["!pip install langchain langchain_groq langchain_community"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10925,"status":"ok","timestamp":1725414486731,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"TgKIrwXJAjgA","outputId":"3670226a-6586-471d-ff73-ac78d09c9c5c"},"outputs":[],"source":["from google.colab import userdata\n","groq_api_key=userdata.get('groq_api_key')\n","langsmith=userdata.get('LANGSMITH_API_KEY')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":656,"status":"ok","timestamp":1725414656746,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"YFpKBPoZBQAa"},"outputs":[],"source":["import os\n","os.environ[\"LANGCHAIN_API_KEY\"] = langsmith\n","os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n","os.environ[\"LANGCHAIN_PROJECT\"]=\"ProjectLangGraph\""]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":583,"status":"ok","timestamp":1725414664614,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"XJZl9uX-CFo7"},"outputs":[],"source":["from langchain_groq import ChatGroq"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":972,"status":"ok","timestamp":1725414684697,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"j9vOWf8yCJ4W","outputId":"f929fa71-3f40-4cb0-ee7b-7c72cc56a36d"},"outputs":[{"data":{"text/plain":["ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x79e3d008d450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x79e3d008e110>, model_name='Gemma2-9b-It', groq_api_key=SecretStr('**********'))"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Gemma2-9b-It\")\n","llm"]},{"cell_type":"markdown","metadata":{"id":"RDc7RviFCVKB"},"source":["## Start Building Chatbot Using LangGraph"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":582,"status":"ok","timestamp":1725414728145,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"bPcSBey6CUeF"},"outputs":[],"source":["from typing import Annotated\n","from typing_extensions import TypedDict\n","from langgraph.graph import StateGraph,START,END\n","from langgraph.graph.message import add_messages"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":298,"status":"ok","timestamp":1725414849466,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"dVzn-8rrC6-7"},"outputs":[],"source":["class State(TypedDict):\n","  # Messages have the type \"list\". The `add_messages` function\n","    # in the annotation defines how this state key should be updated\n","    # (in this case, it appends messages to the list, rather than overwriting them)\n","  messages:Annotated[list,add_messages]\n","\n","graph_builder=StateGraph(State)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1725414855176,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"vgjwX6BQDcfk","outputId":"618ab52c-05f3-4ae0-9ba9-ea21573e8ccc"},"outputs":[{"data":{"text/plain":["<langgraph.graph.state.StateGraph at 0x79e3bbff25c0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["graph_builder"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":287,"status":"ok","timestamp":1725414884976,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"oOF74t5GDeJO"},"outputs":[],"source":["def chatbot(state:State):\n","  return {\"messages\":llm.invoke(state['messages'])}"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":306,"status":"ok","timestamp":1725414931353,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"-m7NDe74EPGW"},"outputs":[],"source":["graph_builder.add_node(\"chatbot\",chatbot)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1725414943136,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"pKFbmbqcEVIq","outputId":"b0777516-81a8-4db5-d472-bc637767f204"},"outputs":[{"data":{"text/plain":["<langgraph.graph.state.StateGraph at 0x79e3bbff25c0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["graph_builder"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":285,"status":"ok","timestamp":1725414973298,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"3jBWX44IEWy-"},"outputs":[],"source":["graph_builder.add_edge(START,\"chatbot\")\n","graph_builder.add_edge(\"chatbot\",END)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":597,"status":"ok","timestamp":1725415003263,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"PN4O63shEtyZ"},"outputs":[],"source":["graph=graph_builder.compile()"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1725415016888,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"_PPER3gfEygm","outputId":"c0142737-66de-453e-b39b-274e4c62b1b7"},"outputs":[{"data":{"image/jpeg":"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","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import Image, display\n","try:\n","  display(Image(graph.get_graph().draw_mermaid_png()))\n","except Exception:\n","  pass"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69519,"status":"ok","timestamp":1725415209125,"user":{"displayName":"Anish Kumar","userId":"11034422725154024399"},"user_tz":240},"id":"aDvjUFmxFCwU","outputId":"6fda9d93-f305-46ed-aba0-b977fba2ea12"},"outputs":[{"name":"stdout","output_type":"stream","text":["User: Hello\n","dict_values([{'messages': AIMessage(content='Hello! üëã\\n\\nHow can I help you today?üòä\\n', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 10, 'total_tokens': 25, 'completion_time': 0.027272727, 'prompt_time': 3.9e-07, 'queue_time': 0.013583028, 'total_time': 0.027273117}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0f78a1af-5e64-4b3f-901d-2cb80a528c1c-0', usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25})}])\n","content='Hello! üëã\\n\\nHow can I help you today?üòä\\n' response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 10, 'total_tokens': 25, 'completion_time': 0.027272727, 'prompt_time': 3.9e-07, 'queue_time': 0.013583028, 'total_time': 0.027273117}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-0f78a1af-5e64-4b3f-901d-2cb80a528c1c-0' usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25}\n","Assistant: Hello! üëã\n","\n","How can I help you today?üòä\n","\n","User: what is generative ai\n","dict_values([{'messages': AIMessage(content='Generative AI is a type of artificial intelligence that focuses on creating new content. \\n\\nInstead of simply analyzing existing data like many other AI models, generative AI learns the underlying patterns and structures within that data and then uses that knowledge to generate new, original outputs. \\n\\nThink of it like this:\\n\\n* **Traditional AI:**  Analyzes a dataset of cat pictures to identify features that make a cat a cat ( pointy ears, whiskers, etc.)\\n* **Generative AI:**  Learns from the cat pictures and can then generate a brand new, unique image of a cat that wasn\\'t in the original dataset.\\n\\n**Here are some key things to know about generative AI:**\\n\\n* **Types of Content:** It can generate various types of content, including:\\n    * **Text:**  Stories, poems, articles, dialogue\\n    * **Images:** Photos, artwork, illustrations\\n    * **Audio:** Music, sound effects, voiceovers\\n    * **Video:** Short clips, animations\\n    * **Code:**  Software programs, scripts\\n* **How it Works:**  Generative AI models are often based on deep learning algorithms, particularly a type called \"generative adversarial networks\" (GANs). These models learn by competing against each other. One model generates content, while another evaluates its quality. Through this back-and-forth, both models improve, leading to increasingly realistic and sophisticated outputs.\\n* **Applications:** Generative AI has many potential applications, including:\\n    * **Creative Industries:**  Art, music, writing\\n    * **Marketing and Advertising:**  Generating personalized content, creating ad campaigns\\n    * **Education:**  Developing interactive learning materials\\n    * **Healthcare:**  Creating synthetic medical images for training\\n    * **Research:**  Exploring new scientific concepts\\n\\n**Examples of Generative AI:**\\n\\n* **DALL-E 2:**  Generates images from text descriptions.\\n* **ChatGPT:**  Generates human-like text in response to prompts.\\n* **Jukebox:**  Generates music in different styles.\\n* **GitHub Copilot:**  Generates code snippets based on comments.\\n\\n\\nGenerative AI is a rapidly evolving field with the potential to revolutionize many aspects of our lives. \\n', response_metadata={'token_usage': {'completion_tokens': 469, 'prompt_tokens': 13, 'total_tokens': 482, 'completion_time': 0.852727273, 'prompt_time': 7.2138e-05, 'queue_time': 0.013401149000000001, 'total_time': 0.852799411}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-35a9efe6-6860-49e8-bd51-85ec45d88b99-0', usage_metadata={'input_tokens': 13, 'output_tokens': 469, 'total_tokens': 482})}])\n","content='Generative AI is a type of artificial intelligence that focuses on creating new content. \\n\\nInstead of simply analyzing existing data like many other AI models, generative AI learns the underlying patterns and structures within that data and then uses that knowledge to generate new, original outputs. \\n\\nThink of it like this:\\n\\n* **Traditional AI:**  Analyzes a dataset of cat pictures to identify features that make a cat a cat ( pointy ears, whiskers, etc.)\\n* **Generative AI:**  Learns from the cat pictures and can then generate a brand new, unique image of a cat that wasn\\'t in the original dataset.\\n\\n**Here are some key things to know about generative AI:**\\n\\n* **Types of Content:** It can generate various types of content, including:\\n    * **Text:**  Stories, poems, articles, dialogue\\n    * **Images:** Photos, artwork, illustrations\\n    * **Audio:** Music, sound effects, voiceovers\\n    * **Video:** Short clips, animations\\n    * **Code:**  Software programs, scripts\\n* **How it Works:**  Generative AI models are often based on deep learning algorithms, particularly a type called \"generative adversarial networks\" (GANs). These models learn by competing against each other. One model generates content, while another evaluates its quality. Through this back-and-forth, both models improve, leading to increasingly realistic and sophisticated outputs.\\n* **Applications:** Generative AI has many potential applications, including:\\n    * **Creative Industries:**  Art, music, writing\\n    * **Marketing and Advertising:**  Generating personalized content, creating ad campaigns\\n    * **Education:**  Developing interactive learning materials\\n    * **Healthcare:**  Creating synthetic medical images for training\\n    * **Research:**  Exploring new scientific concepts\\n\\n**Examples of Generative AI:**\\n\\n* **DALL-E 2:**  Generates images from text descriptions.\\n* **ChatGPT:**  Generates human-like text in response to prompts.\\n* **Jukebox:**  Generates music in different styles.\\n* **GitHub Copilot:**  Generates code snippets based on comments.\\n\\n\\nGenerative AI is a rapidly evolving field with the potential to revolutionize many aspects of our lives. \\n' response_metadata={'token_usage': {'completion_tokens': 469, 'prompt_tokens': 13, 'total_tokens': 482, 'completion_time': 0.852727273, 'prompt_time': 7.2138e-05, 'queue_time': 0.013401149000000001, 'total_time': 0.852799411}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-35a9efe6-6860-49e8-bd51-85ec45d88b99-0' usage_metadata={'input_tokens': 13, 'output_tokens': 469, 'total_tokens': 482}\n","Assistant: Generative AI is a type of artificial intelligence that focuses on creating new content. \n","\n","Instead of simply analyzing existing data like many other AI models, generative AI learns the underlying patterns and structures within that data and then uses that knowledge to generate new, original outputs. \n","\n","Think of it like this:\n","\n","* **Traditional AI:**  Analyzes a dataset of cat pictures to identify features that make a cat a cat ( pointy ears, whiskers, etc.)\n","* **Generative AI:**  Learns from the cat pictures and can then generate a brand new, unique image of a cat that wasn't in the original dataset.\n","\n","**Here are some key things to know about generative AI:**\n","\n","* **Types of Content:** It can generate various types of content, including:\n","    * **Text:**  Stories, poems, articles, dialogue\n","    * **Images:** Photos, artwork, illustrations\n","    * **Audio:** Music, sound effects, voiceovers\n","    * **Video:** Short clips, animations\n","    * **Code:**  Software programs, scripts\n","* **How it Works:**  Generative AI models are often based on deep learning algorithms, particularly a type called \"generative adversarial networks\" (GANs). These models learn by competing against each other. One model generates content, while another evaluates its quality. Through this back-and-forth, both models improve, leading to increasingly realistic and sophisticated outputs.\n","* **Applications:** Generative AI has many potential applications, including:\n","    * **Creative Industries:**  Art, music, writing\n","    * **Marketing and Advertising:**  Generating personalized content, creating ad campaigns\n","    * **Education:**  Developing interactive learning materials\n","    * **Healthcare:**  Creating synthetic medical images for training\n","    * **Research:**  Exploring new scientific concepts\n","\n","**Examples of Generative AI:**\n","\n","* **DALL-E 2:**  Generates images from text descriptions.\n","* **ChatGPT:**  Generates human-like text in response to prompts.\n","* **Jukebox:**  Generates music in different styles.\n","* **GitHub Copilot:**  Generates code snippets based on comments.\n","\n","\n","Generative AI is a rapidly evolving field with the potential to revolutionize many aspects of our lives. \n","\n","User: how does it work\n","dict_values([{'messages': AIMessage(content='Please provide me with more context! \\n\\n\"It\" could refer to many things.  \\n\\nFor example, are you asking about:\\n\\n* **How I work (as a large language model)?**\\n* **How a specific device or technology works?** (e.g., a refrigerator, the internet, a car)\\n* **How a process or concept works?** (e.g., photosynthesis, democracy, baking a cake)\\n\\n\\nOnce you tell me what \"it\" is, I can give you a helpful explanation! üòä \\n', response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 13, 'total_tokens': 129, 'completion_time': 0.210909091, 'prompt_time': 6.9868e-05, 'queue_time': 0.013360661999999999, 'total_time': 0.210978959}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-97aa4ac6-bad2-4d3e-8fa5-4eec4853c718-0', usage_metadata={'input_tokens': 13, 'output_tokens': 116, 'total_tokens': 129})}])\n","content='Please provide me with more context! \\n\\n\"It\" could refer to many things.  \\n\\nFor example, are you asking about:\\n\\n* **How I work (as a large language model)?**\\n* **How a specific device or technology works?** (e.g., a refrigerator, the internet, a car)\\n* **How a process or concept works?** (e.g., photosynthesis, democracy, baking a cake)\\n\\n\\nOnce you tell me what \"it\" is, I can give you a helpful explanation! üòä \\n' response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 13, 'total_tokens': 129, 'completion_time': 0.210909091, 'prompt_time': 6.9868e-05, 'queue_time': 0.013360661999999999, 'total_time': 0.210978959}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-97aa4ac6-bad2-4d3e-8fa5-4eec4853c718-0' usage_metadata={'input_tokens': 13, 'output_tokens': 116, 'total_tokens': 129}\n","Assistant: Please provide me with more context! \n","\n","\"It\" could refer to many things.  \n","\n","For example, are you asking about:\n","\n","* **How I work (as a large language model)?**\n","* **How a specific device or technology works?** (e.g., a refrigerator, the internet, a car)\n","* **How a process or concept works?** (e.g., photosynthesis, democracy, baking a cake)\n","\n","\n","Once you tell me what \"it\" is, I can give you a helpful explanation! üòä \n","\n","User: How large language model works\n","dict_values([{'messages': AIMessage(content='Let\\'s break down how large language models (LLMs) work:\\n\\n**1. The Foundation: Transformer Architecture**\\n\\nAt the heart of most LLMs is a sophisticated neural network architecture called the **Transformer**.  Think of a transformer as a powerful engine designed to understand and generate text.\\n\\n* **Attention is Key:** Transformers excel at \"paying attention\" to different parts of a text input.  Imagine you\\'re reading a sentence. Your brain focuses on certain words more than others to grasp the meaning. Transformers do something similar. They learn to weigh the importance of each word in a sentence, allowing them to capture complex relationships and context.\\n* **Layers of Understanding:** Transformers are built in layers. Each layer processes the input text, refining its understanding at each step. This layered structure allows them to learn intricate patterns and representations of language.\\n\\n**2. Training: Learning from Massive Datasets**\\n\\nLLMs are trained on colossal amounts of text data. This data can include books, articles, websites, code, and more.\\n\\n* **Predicting the Next Word:** The training process involves a simple but powerful idea: predicting the next word in a sequence.  The model is fed a piece of text and must guess what word comes next.  It does this by analyzing the patterns and relationships it has learned from the training data.\\n* **Fine-Tuning:** After initial training, LLMs are often fine-tuned on specific tasks, such as:\\n    * **Text Generation:** Writing stories, poems, articles, or code.\\n    * **Translation:** Converting text from one language to another.\\n    * **Question Answering:** Providing answers to questions based on a given context.\\n    * **Summarization:** Condensing large amounts of text into shorter summaries.\\n\\n**3. How LLMs Generate Text**\\n\\nWhen you give an LLM a prompt (a starting point for text), here\\'s what happens:\\n\\n* **Tokenization:** The prompt is broken down into individual units called tokens (words or parts of words).\\n* **Encoding:** The tokens are converted into numerical representations that the model can understand.\\n* **Processing through Layers:** The encoded tokens are passed through the transformer\\'s layers. At each layer, the model analyzes the relationships between the tokens and updates its understanding of the context.\\n* **Decoding:** The model generates a sequence of probability distributions over the next possible tokens.\\n* **Output:** The tokens with the highest probabilities are selected to form the generated text.\\n\\n**Key Points**\\n\\n* **Scale Matters:** LLMs are known for their massive size, with billions or even trillions of parameters. The more parameters, the greater the model\\'s capacity to learn complex language patterns.\\n* **Data is Crucial:** The quality and quantity of training data directly influence an LLM\\'s performance.\\n* **Open-Weight Models:**  There\\'s a growing trend towards releasing the weights (parameters) of LLMs, making them accessible for research and development by the wider community.\\n\\n\\nLet me know if you\\'d like to explore any of these aspects in more detail!\\n', response_metadata={'token_usage': {'completion_tokens': 643, 'prompt_tokens': 14, 'total_tokens': 657, 'completion_time': 1.169090909, 'prompt_time': 0.000311221, 'queue_time': 0.071227928, 'total_time': 1.16940213}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-931365ff-b90c-4078-9a49-3600ca564fc5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 643, 'total_tokens': 657})}])\n","content='Let\\'s break down how large language models (LLMs) work:\\n\\n**1. The Foundation: Transformer Architecture**\\n\\nAt the heart of most LLMs is a sophisticated neural network architecture called the **Transformer**.  Think of a transformer as a powerful engine designed to understand and generate text.\\n\\n* **Attention is Key:** Transformers excel at \"paying attention\" to different parts of a text input.  Imagine you\\'re reading a sentence. Your brain focuses on certain words more than others to grasp the meaning. Transformers do something similar. They learn to weigh the importance of each word in a sentence, allowing them to capture complex relationships and context.\\n* **Layers of Understanding:** Transformers are built in layers. Each layer processes the input text, refining its understanding at each step. This layered structure allows them to learn intricate patterns and representations of language.\\n\\n**2. Training: Learning from Massive Datasets**\\n\\nLLMs are trained on colossal amounts of text data. This data can include books, articles, websites, code, and more.\\n\\n* **Predicting the Next Word:** The training process involves a simple but powerful idea: predicting the next word in a sequence.  The model is fed a piece of text and must guess what word comes next.  It does this by analyzing the patterns and relationships it has learned from the training data.\\n* **Fine-Tuning:** After initial training, LLMs are often fine-tuned on specific tasks, such as:\\n    * **Text Generation:** Writing stories, poems, articles, or code.\\n    * **Translation:** Converting text from one language to another.\\n    * **Question Answering:** Providing answers to questions based on a given context.\\n    * **Summarization:** Condensing large amounts of text into shorter summaries.\\n\\n**3. How LLMs Generate Text**\\n\\nWhen you give an LLM a prompt (a starting point for text), here\\'s what happens:\\n\\n* **Tokenization:** The prompt is broken down into individual units called tokens (words or parts of words).\\n* **Encoding:** The tokens are converted into numerical representations that the model can understand.\\n* **Processing through Layers:** The encoded tokens are passed through the transformer\\'s layers. At each layer, the model analyzes the relationships between the tokens and updates its understanding of the context.\\n* **Decoding:** The model generates a sequence of probability distributions over the next possible tokens.\\n* **Output:** The tokens with the highest probabilities are selected to form the generated text.\\n\\n**Key Points**\\n\\n* **Scale Matters:** LLMs are known for their massive size, with billions or even trillions of parameters. The more parameters, the greater the model\\'s capacity to learn complex language patterns.\\n* **Data is Crucial:** The quality and quantity of training data directly influence an LLM\\'s performance.\\n* **Open-Weight Models:**  There\\'s a growing trend towards releasing the weights (parameters) of LLMs, making them accessible for research and development by the wider community.\\n\\n\\nLet me know if you\\'d like to explore any of these aspects in more detail!\\n' response_metadata={'token_usage': {'completion_tokens': 643, 'prompt_tokens': 14, 'total_tokens': 657, 'completion_time': 1.169090909, 'prompt_time': 0.000311221, 'queue_time': 0.071227928, 'total_time': 1.16940213}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-931365ff-b90c-4078-9a49-3600ca564fc5-0' usage_metadata={'input_tokens': 14, 'output_tokens': 643, 'total_tokens': 657}\n","Assistant: Let's break down how large language models (LLMs) work:\n","\n","**1. The Foundation: Transformer Architecture**\n","\n","At the heart of most LLMs is a sophisticated neural network architecture called the **Transformer**.  Think of a transformer as a powerful engine designed to understand and generate text.\n","\n","* **Attention is Key:** Transformers excel at \"paying attention\" to different parts of a text input.  Imagine you're reading a sentence. Your brain focuses on certain words more than others to grasp the meaning. Transformers do something similar. They learn to weigh the importance of each word in a sentence, allowing them to capture complex relationships and context.\n","* **Layers of Understanding:** Transformers are built in layers. Each layer processes the input text, refining its understanding at each step. This layered structure allows them to learn intricate patterns and representations of language.\n","\n","**2. Training: Learning from Massive Datasets**\n","\n","LLMs are trained on colossal amounts of text data. This data can include books, articles, websites, code, and more.\n","\n","* **Predicting the Next Word:** The training process involves a simple but powerful idea: predicting the next word in a sequence.  The model is fed a piece of text and must guess what word comes next.  It does this by analyzing the patterns and relationships it has learned from the training data.\n","* **Fine-Tuning:** After initial training, LLMs are often fine-tuned on specific tasks, such as:\n","    * **Text Generation:** Writing stories, poems, articles, or code.\n","    * **Translation:** Converting text from one language to another.\n","    * **Question Answering:** Providing answers to questions based on a given context.\n","    * **Summarization:** Condensing large amounts of text into shorter summaries.\n","\n","**3. How LLMs Generate Text**\n","\n","When you give an LLM a prompt (a starting point for text), here's what happens:\n","\n","* **Tokenization:** The prompt is broken down into individual units called tokens (words or parts of words).\n","* **Encoding:** The tokens are converted into numerical representations that the model can understand.\n","* **Processing through Layers:** The encoded tokens are passed through the transformer's layers. At each layer, the model analyzes the relationships between the tokens and updates its understanding of the context.\n","* **Decoding:** The model generates a sequence of probability distributions over the next possible tokens.\n","* **Output:** The tokens with the highest probabilities are selected to form the generated text.\n","\n","**Key Points**\n","\n","* **Scale Matters:** LLMs are known for their massive size, with billions or even trillions of parameters. The more parameters, the greater the model's capacity to learn complex language patterns.\n","* **Data is Crucial:** The quality and quantity of training data directly influence an LLM's performance.\n","* **Open-Weight Models:**  There's a growing trend towards releasing the weights (parameters) of LLMs, making them accessible for research and development by the wider community.\n","\n","\n","Let me know if you'd like to explore any of these aspects in more detail!\n","\n","User: q\n","Good Bye\n"]}],"source":["while True:\n","  user_input=input(\"User: \")\n","  if user_input.lower() in [\"quit\",\"q\"]:\n","    print(\"Good Bye\")\n","    break\n","  for event in graph.stream({'messages':(\"user\",user_input)}):\n","    print(event.values())\n","    for value in event.values():\n","      print(value['messages'])\n","      print(\"Assistant:\",value[\"messages\"].content)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
